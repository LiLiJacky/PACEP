{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a962116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "935eb0a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "      <th>High</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adjusted Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14-08-2003</td>\n",
       "      <td>14.65</td>\n",
       "      <td>15.15</td>\n",
       "      <td>1176700</td>\n",
       "      <td>15.15</td>\n",
       "      <td>14.90</td>\n",
       "      <td>3.218931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15-08-2003</td>\n",
       "      <td>14.95</td>\n",
       "      <td>15.00</td>\n",
       "      <td>259600</td>\n",
       "      <td>15.25</td>\n",
       "      <td>15.25</td>\n",
       "      <td>3.294542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18-08-2003</td>\n",
       "      <td>15.20</td>\n",
       "      <td>15.28</td>\n",
       "      <td>47900</td>\n",
       "      <td>15.28</td>\n",
       "      <td>15.25</td>\n",
       "      <td>3.294542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19-08-2003</td>\n",
       "      <td>15.10</td>\n",
       "      <td>15.25</td>\n",
       "      <td>64800</td>\n",
       "      <td>15.25</td>\n",
       "      <td>15.15</td>\n",
       "      <td>3.272936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20-08-2003</td>\n",
       "      <td>15.10</td>\n",
       "      <td>15.20</td>\n",
       "      <td>86300</td>\n",
       "      <td>15.34</td>\n",
       "      <td>15.21</td>\n",
       "      <td>3.285900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date    Low   Open   Volume   High  Close  Adjusted Close\n",
       "0  14-08-2003  14.65  15.15  1176700  15.15  14.90        3.218931\n",
       "1  15-08-2003  14.95  15.00   259600  15.25  15.25        3.294542\n",
       "2  18-08-2003  15.20  15.28    47900  15.28  15.25        3.294542\n",
       "3  19-08-2003  15.10  15.25    64800  15.25  15.15        3.272936\n",
       "4  20-08-2003  15.10  15.20    86300  15.34  15.21        3.285900"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分析单个nasdaq文件\n",
    "\n",
    "# 指定文件路径\n",
    "file_path = './nasdaq/csv/GOOD.csv'\n",
    "\n",
    "# 读取 CSV 文件\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 显示前几行数据\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c2f4ba0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern识别结果的数量: 1226\n"
     ]
    }
   ],
   "source": [
    "# 转换日期为 datetime 格式\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%d-%m-%Y')\n",
    "\n",
    "# 定义事件 A: Close 值下降, 事件 B: Close 值上升\n",
    "df['Close_Change'] = df['Close'].diff()\n",
    "\n",
    "# 初始化状态机识别\n",
    "pattern_start = []\n",
    "pattern_end = []\n",
    "in_pattern = False\n",
    "\n",
    "for i in range(1, len(df)):\n",
    "    # 识别事件A: Close值下降\n",
    "    if df['Close_Change'].iloc[i] < 0:\n",
    "        if not in_pattern:\n",
    "            pattern_start.append(i)\n",
    "        in_pattern = True\n",
    "    # 识别事件B: Close值上升，结束模式匹配\n",
    "    elif df['Close_Change'].iloc[i] > 0 and in_pattern:\n",
    "        pattern_end.append(i)\n",
    "        in_pattern = False\n",
    "\n",
    "# 输出匹配到的模式数量\n",
    "pattern_count = len(pattern_start)\n",
    "print(f'Pattern识别结果的数量: {pattern_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fccd9935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern识别结果的数量: 2324\n",
      "最长的时间区间是从 2004-03-29 00:00:00 到 2004-04-16 00:00:00, 跨度为 18 days 00:00:00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 读取数据\n",
    "file_path = './nasdaq/csv/GOOD.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 转换日期为 datetime 格式\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%d-%m-%Y')\n",
    "\n",
    "# 定义事件 A: Close 值下降, 事件 B: Close 值上升\n",
    "df['Close_Change'] = df['Close'].diff()\n",
    "\n",
    "# 初始化状态机识别\n",
    "pattern_start = []\n",
    "pattern_end = []\n",
    "\n",
    "for i in range(1, len(df)):\n",
    "    # 识别事件A: Close值下降\n",
    "    if df['Close_Change'].iloc[i] < 0:\n",
    "        start_index = i\n",
    "        # skip-till-any-match: 允许跳过多个事件A，直到找到事件B\n",
    "        for j in range(i + 1, len(df)):\n",
    "            if df['Close_Change'].iloc[j] > 0:  # 识别事件B: Close值上升\n",
    "                pattern_start.append(start_index)\n",
    "                pattern_end.append(j)\n",
    "                break\n",
    "\n",
    "# 输出匹配到的模式数量\n",
    "pattern_count = len(pattern_start)\n",
    "print(f'Pattern识别结果的数量: {pattern_count}')\n",
    "\n",
    "# 计算每个模式的时间跨度\n",
    "longest_duration = pd.Timedelta(0)\n",
    "longest_start = None\n",
    "longest_end = None\n",
    "\n",
    "for start, end in zip(pattern_start, pattern_end):\n",
    "    duration = df['Date'].iloc[end] - df['Date'].iloc[start-1]\n",
    "    if duration > longest_duration:\n",
    "        longest_duration = duration\n",
    "        longest_start = df['Date'].iloc[start-1]\n",
    "        longest_end = df['Date'].iloc[end]\n",
    "\n",
    "# 输出最长的时间区间\n",
    "if longest_start is not None and longest_end is not None:\n",
    "    print(f'最长的时间区间是从 {longest_start} 到 {longest_end}, 跨度为 {longest_duration}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2a68861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4301"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5227762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date     Low    Open  Volume    High   Close  Adjusted Close  \\\n",
      "453281 1973-05-03  3.6250  3.6250  2000.0  3.8125  3.6250        2.037348   \n",
      "453282 1973-05-04  3.6250  3.6250  1600.0  3.8125  3.6250        2.037348   \n",
      "453283 1973-05-07  3.6250  3.6250  1600.0  3.8125  3.6250        2.037348   \n",
      "453284 1973-05-08  3.6875  3.6875  1600.0  3.8750  3.6875        2.072475   \n",
      "453285 1973-05-09  3.6875  3.6875  6000.0  3.8750  3.6875        2.072475   \n",
      "\n",
      "        Name  \n",
      "453281  ALCO  \n",
      "453282  ALCO  \n",
      "453283  ALCO  \n",
      "453284  ALCO  \n",
      "453285  ALCO  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 定义CSV文件所在目录\n",
    "directory = './nasdaq/csv'\n",
    "\n",
    "# 初始化一个空的DataFrame用于存放所有文件的数据\n",
    "all_data = pd.DataFrame()\n",
    "\n",
    "# 遍历目录下的所有文件\n",
    "size = 100\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        size -= 1\n",
    "        if size < 0:\n",
    "            break\n",
    "        # 构造文件路径\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # 读取CSV文件\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # 提取文件名（去掉扩展名 .csv）\n",
    "        name = os.path.splitext(filename)[0]\n",
    "        \n",
    "        # 添加新的列 'Name'，列值为文件名\n",
    "        df['Name'] = name\n",
    "        \n",
    "        # 转换 'Date' 列为 datetime 类型\n",
    "        df['Date'] = pd.to_datetime(df['Date'], format='%d-%m-%Y')\n",
    "        \n",
    "        # 将数据拼接到 all_data 中\n",
    "        all_data = pd.concat([all_data, df], ignore_index=True)\n",
    "\n",
    "        \n",
    "# 按照 'Date' 和 'Name' 进行排序\n",
    "all_data = all_data.sort_values(by=['Date', 'Name'])\n",
    "\n",
    "# 输出合并后的数据\n",
    "print(all_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35c4e328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "589134"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7cefea4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# 读取CSV文件，处理错误行\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 18\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path, on_bad_lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# 转换 'Date' 列为 datetime 类型\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/jp_book/lib/python3.11/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/jp_book/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/jp_book/lib/python3.11/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/miniconda3/envs/jp_book/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[0;32m~/miniconda3/envs/jp_book/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1795\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1792\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1793\u001b[0m         new_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index)\n\u001b[0;32m-> 1795\u001b[0m     df \u001b[38;5;241m=\u001b[39m DataFrame(col_dict, columns\u001b[38;5;241m=\u001b[39mcolumns, index\u001b[38;5;241m=\u001b[39mindex)\n\u001b[1;32m   1797\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_rows\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msqueeze \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/jp_book/lib/python3.11/site-packages/pandas/core/frame.py:664\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    658\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    659\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    660\u001b[0m     )\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    663\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 664\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, typ\u001b[38;5;241m=\u001b[39mmanager)\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    666\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmrecords\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmrecords\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/jp_book/lib/python3.11/site-packages/pandas/core/internals/construction.py:443\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mseries\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Series\n\u001b[0;32m--> 443\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m Series(data, index\u001b[38;5;241m=\u001b[39mcolumns, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\n\u001b[1;32m    444\u001b[0m     missing \u001b[38;5;241m=\u001b[39m arrays\u001b[38;5;241m.\u001b[39misna()\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;66;03m# GH10856\u001b[39;00m\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;66;03m# raise ValueError if only scalars in dict\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/jp_book/lib/python3.11/site-packages/pandas/core/series.py:436\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    434\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39m_mgr\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_dict_like(data):\n\u001b[0;32m--> 436\u001b[0m     data, index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_dict(data, index, dtype)\n\u001b[1;32m    437\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    438\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/jp_book/lib/python3.11/site-packages/pandas/core/series.py:541\u001b[0m, in \u001b[0;36mSeries._init_dict\u001b[0;34m(self, data, index, dtype)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;66;03m# Now we just make sure the order is respected, if any\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 541\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mreindex(index, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m s\u001b[38;5;241m.\u001b[39m_mgr, s\u001b[38;5;241m.\u001b[39mindex\n",
      "File \u001b[0;32m~/miniconda3/envs/jp_book/lib/python3.11/site-packages/pandas/core/series.py:5094\u001b[0m, in \u001b[0;36mSeries.reindex\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   5090\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   5091\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m passed as both positional and keyword argument\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5092\u001b[0m         )\n\u001b[1;32m   5093\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index})\n\u001b[0;32m-> 5094\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mreindex(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/jp_book/lib/python3.11/site-packages/pandas/core/generic.py:5277\u001b[0m, in \u001b[0;36mNDFrame.reindex\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   5273\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n\u001b[1;32m   5275\u001b[0m \u001b[38;5;66;03m# if all axes that are requested to reindex are equal, then only copy\u001b[39;00m\n\u001b[1;32m   5276\u001b[0m \u001b[38;5;66;03m# if indicated must have index names equal here as well as values\u001b[39;00m\n\u001b[0;32m-> 5277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m   5278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(axis)\u001b[38;5;241m.\u001b[39midentical(ax)\n\u001b[1;32m   5279\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis, ax \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   5280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ax \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   5281\u001b[0m ):\n\u001b[1;32m   5282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m   5284\u001b[0m \u001b[38;5;66;03m# check if we are a multi reindex\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/jp_book/lib/python3.11/site-packages/pandas/core/generic.py:5278\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   5273\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n\u001b[1;32m   5275\u001b[0m \u001b[38;5;66;03m# if all axes that are requested to reindex are equal, then only copy\u001b[39;00m\n\u001b[1;32m   5276\u001b[0m \u001b[38;5;66;03m# if indicated must have index names equal here as well as values\u001b[39;00m\n\u001b[1;32m   5277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[0;32m-> 5278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(axis)\u001b[38;5;241m.\u001b[39midentical(ax)\n\u001b[1;32m   5279\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis, ax \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   5280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ax \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   5281\u001b[0m ):\n\u001b[1;32m   5282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m   5284\u001b[0m \u001b[38;5;66;03m# check if we are a multi reindex\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/jp_book/lib/python3.11/site-packages/pandas/core/indexes/base.py:5563\u001b[0m, in \u001b[0;36mIndex.identical\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   5551\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   5552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21midentical\u001b[39m(\u001b[38;5;28mself\u001b[39m, other) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m   5553\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5554\u001b[0m \u001b[38;5;124;03m    Similar to equals, but checks that object attributes and types are also equal.\u001b[39;00m\n\u001b[1;32m   5555\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5560\u001b[0m \u001b[38;5;124;03m        otherwise False.\u001b[39;00m\n\u001b[1;32m   5561\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   5562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m-> 5563\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mequals(other)\n\u001b[1;32m   5564\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m   5565\u001b[0m             \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, c, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(other, c, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   5566\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_comparables\n\u001b[1;32m   5567\u001b[0m         )\n\u001b[1;32m   5568\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mtype\u001b[39m(other)\n\u001b[1;32m   5569\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/jp_book/lib/python3.11/site-packages/pandas/core/indexes/base.py:5549\u001b[0m, in \u001b[0;36mIndex.equals\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   5545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_extension_array_dtype(other\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m   5546\u001b[0m     \u001b[38;5;66;03m# All EA-backed Index subclasses override equals\u001b[39;00m\n\u001b[1;32m   5547\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m other\u001b[38;5;241m.\u001b[39mequals(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 5549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m array_equivalent(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values, other\u001b[38;5;241m.\u001b[39m_values)\n",
      "File \u001b[0;32m~/miniconda3/envs/jp_book/lib/python3.11/site-packages/pandas/core/dtypes/missing.py:527\u001b[0m, in \u001b[0;36marray_equivalent\u001b[0;34m(left, right, strict_nan, dtype_equal)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# Slow path when we allow comparing different dtypes.\u001b[39;00m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;66;03m# Object arrays can contain None, NaN and NaT.\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;66;03m# string dtypes must be come to this path for NumPy 1.7.1 compat\u001b[39;00m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m left\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOSU\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m right\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOSU\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;66;03m# Note: `in \"OSU\"` is non-trivially faster than `in [\"O\", \"S\", \"U\"]`\u001b[39;00m\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;66;03m#  or `in (\"O\", \"S\", \"U\")`\u001b[39;00m\n\u001b[0;32m--> 527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _array_equivalent_object(left, right, strict_nan)\n\u001b[1;32m    529\u001b[0m \u001b[38;5;66;03m# NaNs can occur in float and complex arrays.\u001b[39;00m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_float_dtype(left\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mor\u001b[39;00m is_complex_dtype(left\u001b[38;5;241m.\u001b[39mdtype):\n",
      "File \u001b[0;32m~/miniconda3/envs/jp_book/lib/python3.11/site-packages/pandas/core/dtypes/missing.py:572\u001b[0m, in \u001b[0;36m_array_equivalent_object\u001b[0;34m(left, right, strict_nan)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m strict_nan:\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;66;03m# isna considers NaN and None to be equivalent.\u001b[39;00m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m left\u001b[38;5;241m.\u001b[39mflags[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF_CONTIGUOUS\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m right\u001b[38;5;241m.\u001b[39mflags[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF_CONTIGUOUS\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    569\u001b[0m         \u001b[38;5;66;03m# we can improve performance by doing a copy-free ravel\u001b[39;00m\n\u001b[1;32m    570\u001b[0m         \u001b[38;5;66;03m# e.g. in frame_methods.Equals.time_frame_nonunique_equal\u001b[39;00m\n\u001b[1;32m    571\u001b[0m         \u001b[38;5;66;03m#  if we transposed the frames\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m         left \u001b[38;5;241m=\u001b[39m left\u001b[38;5;241m.\u001b[39mravel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    573\u001b[0m         right \u001b[38;5;241m=\u001b[39m right\u001b[38;5;241m.\u001b[39mravel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39marray_equivalent_object(\n\u001b[1;32m    576\u001b[0m         ensure_object(left\u001b[38;5;241m.\u001b[39mravel()), ensure_object(right\u001b[38;5;241m.\u001b[39mravel())\n\u001b[1;32m    577\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 定义CSV文件所在目录\n",
    "directory = './nasdaq/csv'\n",
    "\n",
    "# 初始化一个空的列表用于存放结果\n",
    "results = []\n",
    "\n",
    "# 遍历目录下的所有文件\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        # 构造文件路径\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # 读取CSV文件，处理错误行\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, on_bad_lines='skip')\n",
    "            \n",
    "            # 转换 'Date' 列为 datetime 类型\n",
    "            df['Date'] = pd.to_datetime(df['Date'], format='%d-%m-%Y')\n",
    "            \n",
    "            # 获取第一个和最后一个时间戳\n",
    "            first_timestamp = df['Date'].min()\n",
    "            last_timestamp = df['Date'].max()\n",
    "            \n",
    "            # 将结果添加到 results 列表中\n",
    "            results.append({\n",
    "                'File Name': os.path.splitext(filename)[0],\n",
    "                'First Timestamp': first_timestamp,\n",
    "                'Last Timestamp': last_timestamp\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filename}: {e}\")\n",
    "\n",
    "# 使用 pd.DataFrame 将结果转换为 DataFrame\n",
    "timestamps = pd.DataFrame(results)\n",
    "\n",
    "# 输出结果\n",
    "print(timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "896ff262",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'timestamps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 获取最大的 First Timestamp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m max_first_timestamp \u001b[38;5;241m=\u001b[39m timestamps[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFirst Timestamp\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 输出最大的 First Timestamp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m最大的 First Timestamp 是: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_first_timestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'timestamps' is not defined"
     ]
    }
   ],
   "source": [
    "# 获取最大的 First Timestamp\n",
    "max_first_timestamp = timestamps['First Timestamp'].max()\n",
    "\n",
    "# 输出最大的 First Timestamp\n",
    "print(f\"最大的 First Timestamp 是: {max_first_timestamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d1f9970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980-03-17    56\n",
      "1994-04-04     7\n",
      "1973-02-21     6\n",
      "1999-01-04     6\n",
      "2003-10-07     5\n",
      "              ..\n",
      "1998-05-21     1\n",
      "1993-11-01     1\n",
      "1992-07-01     1\n",
      "2002-09-11     1\n",
      "2010-03-01     1\n",
      "Name: First Timestamp, Length: 1277, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 统计每个 First Timestamp 的数量\n",
    "first_timestamp_counts = timestamps['First Timestamp'].value_counts()\n",
    "\n",
    "# 输出每个 First Timestamp 的数量\n",
    "print(first_timestamp_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "569e0c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date         Low        Open    Volume        High       Close  \\\n",
      "10778  1980-03-17    7.625000    7.875000    8800.0    8.125000    7.625000   \n",
      "172448 1980-03-17    5.062500    5.062500   17600.0    5.250000    5.062500   \n",
      "140114 1980-03-17    1.170000    1.170000     694.0    1.278000    1.170000   \n",
      "64668  1980-03-17    0.320988    0.000000   10125.0    0.345679    0.320988   \n",
      "161670 1980-03-17    5.500000    5.562500  210600.0    5.625000    5.625000   \n",
      "...           ...         ...         ...       ...         ...         ...   \n",
      "32333  2022-12-12    7.465000    7.484800    1415.0    7.488700    7.480000   \n",
      "64667  2022-12-12  172.910004  174.110001  748248.0  174.445007  174.020004   \n",
      "43111  2022-12-12   23.520100   24.170000   12485.0   24.170000   23.770100   \n",
      "86223  2022-12-12   35.340000   35.529999  943656.0   35.840000   35.669998   \n",
      "53889  2022-12-12         NaN         NaN       NaN         NaN         NaN   \n",
      "\n",
      "        Adjusted Close   Name  \n",
      "10778         4.285454   ALCO  \n",
      "172448        4.291082   ALOG  \n",
      "140114        0.597941   APOG  \n",
      "64668         0.320988   DIOD  \n",
      "161670        2.106952     GT  \n",
      "...                ...    ...  \n",
      "32333         7.480000   TSRI  \n",
      "64667       174.020004    TXN  \n",
      "43111        23.770100  VLGEA  \n",
      "86223        35.669998    WDC  \n",
      "53889              NaN   WSCI  \n",
      "\n",
      "[204782 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 定义CSV文件所在目录\n",
    "directory = './nasdaq/csv'\n",
    "\n",
    "# 定义过滤的日期\n",
    "filter_date = pd.to_datetime('1980-03-17')\n",
    "\n",
    "# 初始化一个空的DataFrame用于存放结果\n",
    "filtered_data = pd.DataFrame()\n",
    "\n",
    "# 遍历目录下的所有文件\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        # 构造文件路径\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # 读取CSV文件，并跳过有问题的行\n",
    "        df = pd.read_csv(file_path, on_bad_lines='skip')\n",
    "        \n",
    "        # 转换 'Date' 列为 datetime 类型\n",
    "        df['Date'] = pd.to_datetime(df['Date'], format='%d-%m-%Y', errors='coerce')\n",
    "        \n",
    "        # 检查第一个时间戳是否早于1980-03-17\n",
    "        first_timestamp = df['Date'].min()\n",
    "        if first_timestamp < filter_date:\n",
    "            # 保留1980-03-17之后的数据\n",
    "            df_filtered = df[df['Date'] >= filter_date].copy()  # 使用 .copy() 防止视图问题\n",
    "            \n",
    "            # 添加一个新列 'Name' 以标识文件来源\n",
    "            df_filtered.loc[:, 'Name'] = os.path.splitext(filename)[0]  # 使用 .loc 进行赋值\n",
    "            \n",
    "            # 将结果收集到 filtered_data 中\n",
    "            filtered_data = pd.concat([filtered_data, df_filtered], ignore_index=True)\n",
    "\n",
    "# 对结果先按 'Date' 排序，再按 'Name' 排序\n",
    "filtered_data = filtered_data.sort_values(by=['Date', 'Name'])\n",
    "\n",
    "# 输出结果\n",
    "print(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "625a0bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Stock Name Start Date Matched Stock  \\\n",
      "0         KLIC 1980-03-21        [DIOD]   \n",
      "1         KLIC 1980-03-24        [DIOD]   \n",
      "2         KLIC 1980-03-25        [DIOD]   \n",
      "3         KLIC 1980-03-26        [DIOD]   \n",
      "4         KLIC 1980-03-27        [DIOD]   \n",
      "..         ...        ...           ...   \n",
      "141        WDC 1980-04-25        [DIOD]   \n",
      "142        WDC 1980-04-28        [DIOD]   \n",
      "143        WDC 1980-04-29        [DIOD]   \n",
      "144        WDC 1980-04-30        [DIOD]   \n",
      "145        WDC 1980-05-01        [DIOD]   \n",
      "\n",
      "                                          Matched Date  \n",
      "0                      [1980-04-15T00:00:00.000000000]  \n",
      "1                      [1980-04-15T00:00:00.000000000]  \n",
      "2                      [1980-04-15T00:00:00.000000000]  \n",
      "3                      [1980-04-15T00:00:00.000000000]  \n",
      "4                      [1980-04-15T00:00:00.000000000]  \n",
      "..                                                 ...  \n",
      "141  [1980-04-29T00:00:00.000000000, 1980-05-02T00:...  \n",
      "142  [1980-04-29T00:00:00.000000000, 1980-05-02T00:...  \n",
      "143                    [1980-05-02T00:00:00.000000000]  \n",
      "144                    [1980-05-02T00:00:00.000000000]  \n",
      "145                    [1980-05-02T00:00:00.000000000]  \n",
      "\n",
      "[146 rows x 4 columns]\n",
      "最长时间跨度： 41 days 00:00:00\n",
      "代码运行时间：141.90秒\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# 开始时间\n",
    "start_time = time.time()\n",
    "\n",
    "# 假设 filtered_data 是已经处理过的数据\n",
    "# filtered_data 应包含 'Date', 'Name', 'Open', 'Close', 'High' 等列\n",
    "\n",
    "# 确保 Date 列为 datetime 类型\n",
    "filtered_data['Date'] = pd.to_datetime(filtered_data['Date'])\n",
    "\n",
    "# 将数据按 Name 和 Date 排序\n",
    "filtered_data = filtered_data.sort_values(by=['Name', 'Date'])\n",
    "\n",
    "# 初始化用于存储模式匹配的结果\n",
    "pattern_results = []\n",
    "\n",
    "# 遍历每个股票的组\n",
    "for name, group in filtered_data.groupby('Name'):\n",
    "    group['Up'] = group['Close'] > group['Open']  # 判断是否上涨\n",
    "    group['Up_Streak'] = group['Up'].astype(int).rolling(window=5, min_periods=5).sum()  # 计算连续上涨次数\n",
    "\n",
    "    # 过滤出连续上涨且持续3天以上的日期\n",
    "    valid_streaks = group[(group['Up_Streak'] >= 5) & (group['Up'])]\n",
    "    \n",
    "    if not valid_streaks.empty:\n",
    "        for idx in valid_streaks.index:\n",
    "            # 确定时间窗口的开始日期\n",
    "            start_date = valid_streaks.at[idx, 'Date']\n",
    "            end_date = start_date + pd.Timedelta(days=30)  # 计算30天后的结束日期\n",
    "            \n",
    "            # 检查其他股票在此日期之后的30天内是否有 Close > High\n",
    "            subsequent_dates = filtered_data[(filtered_data['Date'] > start_date) & \n",
    "                                              (filtered_data['Date'] <= end_date) & \n",
    "                                              (filtered_data['Name'] != name)]\n",
    "            if not subsequent_dates.empty:\n",
    "                b_condition_met = subsequent_dates[subsequent_dates['Close'] > subsequent_dates['High']]\n",
    "                if not b_condition_met.empty:\n",
    "                    pattern_results.append({\n",
    "                        'Stock Name': name,\n",
    "                        'Start Date': start_date,\n",
    "                        'Matched Stock': b_condition_met['Name'].unique(),\n",
    "                        'Matched Date': b_condition_met['Date'].unique()\n",
    "                    })\n",
    "\n",
    "# 输出匹配的结果\n",
    "results_df = pd.DataFrame(pattern_results)\n",
    "print(results_df)\n",
    "\n",
    "# 计算最长时间跨度\n",
    "if not results_df.empty:\n",
    "    max_span = results_df['Start Date'].max() - results_df['Start Date'].min()\n",
    "    print(\"最长时间跨度：\", max_span)\n",
    "else:\n",
    "    print(\"未找到符合条件的模式。\")\n",
    "\n",
    "# 结束时间\n",
    "end_time = time.time()\n",
    "print(\"代码运行时间：{:.2f}秒\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c4f91c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1698c2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name\n",
      "ALCO      841\n",
      "ALOG      413\n",
      "APOG      751\n",
      "DIOD     1200\n",
      "GT        428\n",
      "HELE      910\n",
      "KLIC     1102\n",
      "MAT       525\n",
      "MSEX      303\n",
      "OTTR      174\n",
      "PHI       401\n",
      "SGC       496\n",
      "SHLM      388\n",
      "TRNS     2157\n",
      "TSRI     1704\n",
      "TXN       286\n",
      "VLGEA     727\n",
      "WDC      1048\n",
      "WSCI     1354\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 按照每只股票进行分组，计算各个股票的闪崩次数\n",
    "def detect_flash_crash(group):\n",
    "    # 定义条件：价格下跌超过5%\n",
    "    group['Flash_Crash'] = ((group['High'] - group['Low']) / group['High']) >= 0.05\n",
    "    \n",
    "    # 可以根据需要添加其他条件，比如成交量异常增高等\n",
    "    # 比如说当天成交量大于前一天的1.5倍\n",
    "    group['Volume_Increase'] = group['Volume'].pct_change() > 0.5\n",
    "    \n",
    "    # 仅保留满足上述两个条件的行\n",
    "    group['Flash_Crash'] = group['Flash_Crash'] & group['Volume_Increase']\n",
    "    \n",
    "    # 统计闪崩次数\n",
    "    return group['Flash_Crash'].sum()\n",
    "\n",
    "# 应用函数，按股票名称分组\n",
    "flash_crash_counts = filtered_data.groupby('Name').apply(detect_flash_crash)\n",
    "\n",
    "# 查看统计结果\n",
    "print(flash_crash_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8805e5d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input y contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# 运行函数\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m result \u001b[38;5;241m=\u001b[39m detect_significant_upward_trend(filtered_data)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# 结束时间\u001b[39;00m\n\u001b[1;32m     48\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[6], line 30\u001b[0m, in \u001b[0;36mdetect_significant_upward_trend\u001b[0;34m(stock_data, min_days, max_days, slope_threshold)\u001b[0m\n\u001b[1;32m     28\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(selected_prices))\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     29\u001b[0m model \u001b[38;5;241m=\u001b[39m LinearRegression()\n\u001b[0;32m---> 30\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X, selected_prices)\n\u001b[1;32m     31\u001b[0m slope \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcoef_[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# 获取斜率\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# 如果斜率大于2，认为是显著上升\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/jp_book/lib/python3.11/site-packages/sklearn/linear_model/_base.py:648\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    644\u001b[0m n_jobs_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs\n\u001b[1;32m    646\u001b[0m accept_sparse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositive \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 648\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    649\u001b[0m     X, y, accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse, y_numeric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, multi_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    650\u001b[0m )\n\u001b[1;32m    652\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(\n\u001b[1;32m    653\u001b[0m     sample_weight, X, dtype\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mdtype, only_non_negative\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    654\u001b[0m )\n\u001b[1;32m    656\u001b[0m X, y, X_offset, y_offset, X_scale \u001b[38;5;241m=\u001b[39m _preprocess_data(\n\u001b[1;32m    657\u001b[0m     X,\n\u001b[1;32m    658\u001b[0m     y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    661\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m    662\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/jp_book/lib/python3.11/site-packages/sklearn/base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 584\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m    585\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniconda3/envs/jp_book/lib/python3.11/site-packages/sklearn/utils/validation.py:1122\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1104\u001b[0m     )\n\u001b[1;32m   1106\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m   1107\u001b[0m     X,\n\u001b[1;32m   1108\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1119\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1120\u001b[0m )\n\u001b[0;32m-> 1122\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1124\u001b[0m check_consistent_length(X, y)\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m~/miniconda3/envs/jp_book/lib/python3.11/site-packages/sklearn/utils/validation.py:1132\u001b[0m, in \u001b[0;36m_check_y\u001b[0;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Isolated part of check_X_y dedicated to y validation\"\"\"\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multi_output:\n\u001b[0;32m-> 1132\u001b[0m     y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m   1133\u001b[0m         y,\n\u001b[1;32m   1134\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1135\u001b[0m         force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1136\u001b[0m         ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1137\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1138\u001b[0m         input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1139\u001b[0m         estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[1;32m   1140\u001b[0m     )\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1142\u001b[0m     estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n",
      "File \u001b[0;32m~/miniconda3/envs/jp_book/lib/python3.11/site-packages/sklearn/utils/validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    916\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    917\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    918\u001b[0m         )\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 921\u001b[0m         _assert_all_finite(\n\u001b[1;32m    922\u001b[0m             array,\n\u001b[1;32m    923\u001b[0m             input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m    924\u001b[0m             estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m    925\u001b[0m             allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    926\u001b[0m         )\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    929\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/miniconda3/envs/jp_book/lib/python3.11/site-packages/sklearn/utils/validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m     )\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input y contains NaN."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "# 假设 filtered_data 是你的输入数据\n",
    "# filtered_data 应包含 'Date', 'Close', 'Name' 等字段\n",
    "\n",
    "def detect_significant_upward_trend(stock_data, min_days=5, max_days=10, slope_threshold=2):\n",
    "    stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n",
    "    stock_data.sort_values(by='Date', inplace=True)\n",
    "    results = []\n",
    "\n",
    "    unique_stocks = stock_data['Name'].unique()\n",
    "\n",
    "    for stock in unique_stocks:\n",
    "        current_stock_data = stock_data[stock_data['Name'] == stock]\n",
    "        close_prices = current_stock_data['Close'].values\n",
    "\n",
    "        # 提取30天内的所有组合\n",
    "        if len(close_prices) >= max_days:\n",
    "            for i in range(len(close_prices) - max_days + 1):\n",
    "                for n in range(min_days, max_days + 1):\n",
    "                    for comb in combinations(range(i, i + max_days), n):\n",
    "                        # 获取组合对应的收盘价\n",
    "                        selected_prices = close_prices[list(comb)]\n",
    "\n",
    "                        # 线性回归计算斜率\n",
    "                        X = np.arange(len(selected_prices)).reshape(-1, 1)\n",
    "                        model = LinearRegression()\n",
    "                        model.fit(X, selected_prices)\n",
    "                        slope = model.coef_[0]  # 获取斜率\n",
    "\n",
    "                        # 如果斜率大于2，认为是显著上升\n",
    "                        if slope > slope_threshold:\n",
    "                            results.append((stock, selected_prices, slope))\n",
    "\n",
    "    return results\n",
    "\n",
    "flash_crash_data = filtered_data[filtered_data['Name'] == \"ALCO\"]\n",
    "\n",
    "# 开始时间\n",
    "start_time = time.time()\n",
    "\n",
    "# 运行函数\n",
    "result = detect_significant_upward_trend(filtered_data)\n",
    "\n",
    "# 结束时间\n",
    "end_time = time.time()\n",
    "print(\"代码运行时间：{:.2f}秒\".format(end_time - start_time))\n",
    "\n",
    "# 打印结果\n",
    "for stock, prices, slope in result:\n",
    "    print(f'Stock: {stock}, Prices: {prices}, Slope: {slope}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22ae9da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "代码运行时间：6.15秒\n",
      "匹配的结果数量: 114333\n",
      "结果已保存到 result.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from itertools import combinations\n",
    "\n",
    "def calculate_slope(dates, prices):\n",
    "    \"\"\"\n",
    "    计算给定日期和价格的斜率\n",
    "    使用最小二乘法公式计算斜率：\n",
    "    slope = (N*Σxy - Σx*Σy) / (N*Σx^2 - (Σx)^2)\n",
    "    \"\"\"\n",
    "    N = len(dates)\n",
    "    sum_x = np.sum(dates)\n",
    "    sum_y = np.sum(prices)\n",
    "    sum_xy = np.sum(dates * prices)\n",
    "    sum_x_squared = np.sum(dates ** 2)\n",
    "\n",
    "    numerator = N * sum_xy - sum_x * sum_y\n",
    "    denominator = N * sum_x_squared - sum_x ** 2\n",
    "\n",
    "    return numerator / denominator if denominator != 0 else 0\n",
    "\n",
    "def detect_flash_crash(stock_data, min_days=5, max_days=10, slope_threshold=0.1, crash_threshold=0.5):\n",
    "    \"\"\"\n",
    "    检测股票数据中的闪崩事件，只遍历一次数据。\n",
    "    \"\"\"\n",
    "    stock_data = stock_data.copy()\n",
    "    stock_data['Date'] = pd.to_datetime(stock_data['Date'], format='%Y-%m-%d')\n",
    "    stock_data.sort_values(by='Date', inplace=True)\n",
    "    stock_data['Date'] = stock_data['Date'].map(pd.Timestamp.toordinal)\n",
    "\n",
    "    result_list = []\n",
    "    unique_stocks = stock_data['Name'].unique()\n",
    "\n",
    "    for stock in unique_stocks:\n",
    "        current_stock_data = stock_data[stock_data['Name'] == stock]\n",
    "        close_prices = current_stock_data['Close'].values\n",
    "        low_prices = current_stock_data['Low'].values\n",
    "        high_prices = current_stock_data['High'].values\n",
    "        dates = current_stock_data['Date'].values\n",
    "\n",
    "        # 遍历每一个可能的崩溃事件\n",
    "        for j in range(len(close_prices)):\n",
    "            price_drop = high_prices[j] - low_prices[j]\n",
    "            if price_drop >= crash_threshold:\n",
    "                crash_date = dates[j]\n",
    "\n",
    "                # 查找崩溃日期前10天内的数据\n",
    "                flash_candidates = dates[max(0, j - 9):j]\n",
    "                candidate_prices = close_prices[max(0, j - 9):j]\n",
    "                \n",
    "                if len(flash_candidates) < 5:\n",
    "                    continue\n",
    "\n",
    "                # 判断是否满足min和max条件\n",
    "                if min(candidate_prices) < low_prices[j] and max(candidate_prices) > high_prices[j]:\n",
    "                    # 满足min和max条件后生成组合\n",
    "                    for n in range(min_days, min(max_days, len(flash_candidates)) + 1):\n",
    "                        for comb in combinations(range(len(flash_candidates)), n):\n",
    "                            selected_dates = flash_candidates[list(comb)]\n",
    "                            selected_prices = candidate_prices[list(comb)]\n",
    "\n",
    "                            # 计算斜率\n",
    "                            slope = calculate_slope(selected_dates, selected_prices)\n",
    "                            if slope > slope_threshold:\n",
    "                                begin_date = pd.Timestamp.fromordinal(selected_dates[0]).strftime('%Y-%m-%d')\n",
    "                                end_date = pd.Timestamp.fromordinal(crash_date).strftime('%Y-%m-%d')\n",
    "\n",
    "                                flash_stage = [pd.Timestamp.fromordinal(ts).strftime('%Y-%m-%d') \n",
    "                                               for ts in selected_dates]\n",
    "\n",
    "                                result_list.append({\n",
    "                                    \"begin_date\": begin_date,\n",
    "                                    \"end_date\": end_date,\n",
    "                                    \"flash_stage\": \";\".join(flash_stage),\n",
    "                                    \"crash_stage\": pd.Timestamp.fromordinal(crash_date).strftime('%Y-%m-%d')\n",
    "                                })\n",
    "\n",
    "    return result_list\n",
    "\n",
    "# 假设 filtered_data 是输入数据\n",
    "flash_crash_data = filtered_data[filtered_data['Name'] == \"ALCO\"]\n",
    "\n",
    "start_time = time.time()\n",
    "result = detect_flash_crash(flash_crash_data)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"代码运行时间：{end_time - start_time:.2f}秒\")\n",
    "print(f\"匹配的结果数量: {len(result)}\")\n",
    "\n",
    "# 将结果转换为DataFrame并保存为CSV文件\n",
    "df = pd.DataFrame(result)\n",
    "df.to_csv(\"result.csv\", index=False)\n",
    "\n",
    "print(\"结果已保存到 result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47ef730",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0dcc5a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'unique'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(df\u001b[38;5;241m.\u001b[39munique())\n",
      "File \u001b[0;32m~/miniconda3/envs/jp_book/lib/python3.11/site-packages/pandas/core/generic.py:5902\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5895\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5896\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[1;32m   5897\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[1;32m   5898\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[1;32m   5899\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5900\u001b[0m ):\n\u001b[1;32m   5901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[0;32m-> 5902\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'unique'"
     ]
    }
   ],
   "source": [
    "len(df.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3581a831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "      <th>High</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adjusted Close</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10778</th>\n",
       "      <td>1980-03-17</td>\n",
       "      <td>7.625</td>\n",
       "      <td>7.875</td>\n",
       "      <td>8800.0</td>\n",
       "      <td>8.125</td>\n",
       "      <td>7.625</td>\n",
       "      <td>4.285454</td>\n",
       "      <td>ALCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10779</th>\n",
       "      <td>1980-03-18</td>\n",
       "      <td>7.625</td>\n",
       "      <td>7.625</td>\n",
       "      <td>5200.0</td>\n",
       "      <td>8.125</td>\n",
       "      <td>7.625</td>\n",
       "      <td>4.285454</td>\n",
       "      <td>ALCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10780</th>\n",
       "      <td>1980-03-19</td>\n",
       "      <td>7.500</td>\n",
       "      <td>7.625</td>\n",
       "      <td>12400.0</td>\n",
       "      <td>8.000</td>\n",
       "      <td>7.500</td>\n",
       "      <td>4.215203</td>\n",
       "      <td>ALCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10781</th>\n",
       "      <td>1980-03-20</td>\n",
       "      <td>7.375</td>\n",
       "      <td>7.500</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>7.875</td>\n",
       "      <td>7.375</td>\n",
       "      <td>4.144949</td>\n",
       "      <td>ALCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10782</th>\n",
       "      <td>1980-03-21</td>\n",
       "      <td>7.375</td>\n",
       "      <td>7.375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.375</td>\n",
       "      <td>7.375</td>\n",
       "      <td>4.144949</td>\n",
       "      <td>ALCO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date    Low   Open   Volume   High  Close  Adjusted Close  Name\n",
       "10778 1980-03-17  7.625  7.875   8800.0  8.125  7.625        4.285454  ALCO\n",
       "10779 1980-03-18  7.625  7.625   5200.0  8.125  7.625        4.285454  ALCO\n",
       "10780 1980-03-19  7.500  7.625  12400.0  8.000  7.500        4.215203  ALCO\n",
       "10781 1980-03-20  7.375  7.500   1600.0  7.875  7.375        4.144949  ALCO\n",
       "10782 1980-03-21  7.375  7.375      0.0  7.375  7.375        4.144949  ALCO"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flash_crash_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "adffe021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming flash_crash_data is a pandas DataFrame\n",
    "flash_crash_data.to_csv('ALCO.csv', index=False)  # Export to CSV without the index column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "12d0c2ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "      <th>High</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adjusted Close</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10778</th>\n",
       "      <td>1980-03-17</td>\n",
       "      <td>7.625</td>\n",
       "      <td>7.875</td>\n",
       "      <td>8800.0</td>\n",
       "      <td>8.125</td>\n",
       "      <td>7.625</td>\n",
       "      <td>4.285454</td>\n",
       "      <td>ALCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10779</th>\n",
       "      <td>1980-03-18</td>\n",
       "      <td>7.625</td>\n",
       "      <td>7.625</td>\n",
       "      <td>5200.0</td>\n",
       "      <td>8.125</td>\n",
       "      <td>7.625</td>\n",
       "      <td>4.285454</td>\n",
       "      <td>ALCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10780</th>\n",
       "      <td>1980-03-19</td>\n",
       "      <td>7.500</td>\n",
       "      <td>7.625</td>\n",
       "      <td>12400.0</td>\n",
       "      <td>8.000</td>\n",
       "      <td>7.500</td>\n",
       "      <td>4.215203</td>\n",
       "      <td>ALCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10781</th>\n",
       "      <td>1980-03-20</td>\n",
       "      <td>7.375</td>\n",
       "      <td>7.500</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>7.875</td>\n",
       "      <td>7.375</td>\n",
       "      <td>4.144949</td>\n",
       "      <td>ALCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10782</th>\n",
       "      <td>1980-03-21</td>\n",
       "      <td>7.375</td>\n",
       "      <td>7.375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.375</td>\n",
       "      <td>7.375</td>\n",
       "      <td>4.144949</td>\n",
       "      <td>ALCO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date    Low   Open   Volume   High  Close  Adjusted Close  Name\n",
       "10778 1980-03-17  7.625  7.875   8800.0  8.125  7.625        4.285454  ALCO\n",
       "10779 1980-03-18  7.625  7.625   5200.0  8.125  7.625        4.285454  ALCO\n",
       "10780 1980-03-19  7.500  7.625  12400.0  8.000  7.500        4.215203  ALCO\n",
       "10781 1980-03-20  7.375  7.500   1600.0  7.875  7.375        4.144949  ALCO\n",
       "10782 1980-03-21  7.375  7.375      0.0  7.375  7.375        4.144949  ALCO"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flash_crash_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74c09963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'begin_date': '1985-08-26', 'end_date': '1985-09-04', 'flash_stage': '1985-08-26;1985-08-27;1985-08-28;1985-08-29;1985-09-03', 'crash_stage': '1985-09-04'}\n"
     ]
    }
   ],
   "source": [
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a84c354",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10213772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4268"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "55360eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始 result_df 行数: 866\n",
      "原始 latency_df 行数: 4794\n",
      "重叠的部分行数（交集）: 100\n",
      "只在 result_df 中的部分行数: 766\n",
      "只在 latency_df 中的部分行数: 4694\n",
      "合计行数: 5560\n",
      "预期行数（result_df + latency_df - overlap_df）: 5560\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9cd8e1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-12;2022-04-13;2022-04-14;2022-04-18;2022-04-19;2022-04-20;2022-04-21\n",
      "2022-10-10;2022-10-11;2022-10-12;2022-10-13;2022-10-17\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7697fae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "事件检测结果：满足事件条件\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "20aec320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[724879 724880 724881 724882 724883]\n",
      "0.1625\n",
      "[724879 724880 724881 724882 724887]\n",
      "0.15206185567010308\n",
      "[724879 724880 724881 724882 724888]\n",
      "0.13\n",
      "[724879 724880 724881 724883 724887]\n",
      "0.1390625\n",
      "[724879 724880 724881 724883 724888]\n",
      "0.12401574803149606\n",
      "[724879 724880 724881 724887 724888]\n",
      "0.13214285714285715\n",
      "[724879 724880 724882 724883 724887]\n",
      "0.1559278350515464\n",
      "[724879 724880 724882 724883 724888]\n",
      "0.13795731707317074\n",
      "[724879 724880 724882 724887 724888]\n",
      "0.14296407185628743\n",
      "[724879 724880 724883 724887 724888]\n",
      "0.15529141104294478\n",
      "[724879 724881 724882 724883 724887]\n",
      "0.11754261363636363\n",
      "[724879 724881 724882 724883 724888]\n",
      "0.103429203539823\n",
      "[724879 724881 724882 724887 724888]\n",
      "0.1053921568627451\n",
      "[724879 724881 724883 724887 724888]\n",
      "0.1176097972972973\n",
      "[724879 724882 724883 724887 724888]\n",
      "0.13138686131386862\n",
      "[724880 724881 724882 724883 724887]\n",
      "0.1172945205479452\n",
      "[724880 724881 724882 724883 724888]\n",
      "0.10212628865979381\n",
      "[724880 724881 724882 724887 724888]\n",
      "0.10432330827067669\n",
      "[724880 724881 724883 724887 724888]\n",
      "0.1220472440944882\n",
      "[724880 724882 724883 724887 724888]\n",
      "0.13994565217391305\n",
      "[724881 724882 724883 724887 724888]\n",
      "0.06378865979381443\n",
      "[724879 724880 724881 724882 724883 724887]\n",
      "0.1390625\n",
      "[724879 724880 724881 724882 724883 724888]\n",
      "0.1223360655737705\n",
      "[724879 724880 724881 724882 724887 724888]\n",
      "0.12705882352941175\n",
      "[724879 724880 724881 724883 724887 724888]\n",
      "0.13214285714285715\n",
      "[724879 724880 724882 724883 724887 724888]\n",
      "0.14385910224438903\n",
      "[724879 724881 724882 724883 724887 724888]\n",
      "0.10835597826086957\n",
      "[724880 724881 724882 724883 724887 724888]\n",
      "0.10922897196261683\n",
      "[724879 724880 724881 724882 724883 724887 724888]\n",
      "0.12613407258064516\n",
      "符合调价的事件检测结果：\n",
      "[{'begin_date': '1985-08-26', 'end_date': '1985-09-04', 'flash_stage': '1985-08-26;1985-08-27;1985-08-28;1985-08-29;1985-09-03', 'crash_stage': '1985-09-04'}, {'begin_date': '1985-08-26', 'end_date': '1985-09-04', 'flash_stage': '1985-08-26;1985-08-27;1985-08-28;1985-08-30;1985-09-03', 'crash_stage': '1985-09-04'}, {'begin_date': '1985-08-26', 'end_date': '1985-09-04', 'flash_stage': '1985-08-26;1985-08-27;1985-08-29;1985-08-30;1985-09-03', 'crash_stage': '1985-09-04'}, {'begin_date': '1985-08-26', 'end_date': '1985-09-04', 'flash_stage': '1985-08-26;1985-08-28;1985-08-29;1985-08-30;1985-09-03', 'crash_stage': '1985-09-04'}, {'begin_date': '1985-08-27', 'end_date': '1985-09-04', 'flash_stage': '1985-08-27;1985-08-28;1985-08-29;1985-08-30;1985-09-03', 'crash_stage': '1985-09-04'}, {'begin_date': '1985-08-26', 'end_date': '1985-09-04', 'flash_stage': '1985-08-26;1985-08-27;1985-08-28;1985-08-29;1985-08-30;1985-09-03', 'crash_stage': '1985-09-04'}]\n",
      "6\n",
      "无重复事件\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "17d0aa01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>begin_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>flash_stage</th>\n",
       "      <th>crash_stage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1985-08-26</td>\n",
       "      <td>1985-09-04</td>\n",
       "      <td>1985-08-26;1985-08-27;1985-08-28;1985-08-29;19...</td>\n",
       "      <td>1985-09-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   begin_date    end_date                                        flash_stage  \\\n",
       "0  1985-08-26  1985-09-04  1985-08-26;1985-08-27;1985-08-28;1985-08-29;19...   \n",
       "\n",
       "  crash_stage  \n",
       "0  1985-09-04  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_to_result_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c15f9af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "持续时间最短的结果为: ('ALCO', array([22.25, 22.25, 22.25, 22.25, 23.25]), 0.16981132075471697, (Timestamp('1986-02-25 00:00:00'), Timestamp('1986-03-04 00:00:00')))\n"
     ]
    }
   ],
   "source": [
    "# 找出持续时间最短的结果\n",
    "shortest_result = min(result, key=lambda x: (x[3][1] - x[3][0]).days)\n",
    "\n",
    "print(\"持续时间最短的结果为:\", shortest_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5b243d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设定日期范围\n",
    "start_date = pd.Timestamp('1986-02-25')\n",
    "end_date = pd.Timestamp('1986-03-04')\n",
    "\n",
    "# 筛选出日期范围内的数据\n",
    "flash_crash_data_succeed_demo = flash_crash_data[\n",
    "    (flash_crash_data['Date'] >= start_date) & (flash_crash_data['Date'] <= end_date)\n",
    "]\n",
    "\n",
    "# 保存为 CSV 文件\n",
    "flash_crash_data_succeed_demo.to_csv(\"flash_crash_data_succeed_demo.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8d41559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n6/_bh3pvzd13s0pgv_hdy0np780000gn/T/ipykernel_40337/404727076.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n",
      "/var/folders/n6/_bh3pvzd13s0pgv_hdy0np780000gn/T/ipykernel_40337/404727076.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stock_data.sort_values(by='Date', inplace=True)\n",
      "/var/folders/n6/_bh3pvzd13s0pgv_hdy0np780000gn/T/ipykernel_40337/404727076.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stock_data['Date'] = stock_data['Date'].map(pd.Timestamp.toordinal)  # 将日期转换为整数\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 70\u001b[0m\n\u001b[1;32m     67\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# 运行函数\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m result \u001b[38;5;241m=\u001b[39m detect_flash_crash(flash_crash_data)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# 结束时间\u001b[39;00m\n\u001b[1;32m     73\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[37], line 45\u001b[0m, in \u001b[0;36mdetect_flash_crash\u001b[0;34m(stock_data, min_days, max_days, slope_threshold, crash_threshold)\u001b[0m\n\u001b[1;32m     42\u001b[0m selected_prices \u001b[38;5;241m=\u001b[39m close_prices[\u001b[38;5;28mlist\u001b[39m(comb)]\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# 计算斜率，检测上升趋势\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m slope \u001b[38;5;241m=\u001b[39m calculate_slope(selected_dates, selected_prices)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# 如果斜率大于2，认为是显著上升\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m slope \u001b[38;5;241m>\u001b[39m slope_threshold:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# 检查上升趋势后的10天内是否出现闪崩\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[37], line 15\u001b[0m, in \u001b[0;36mcalculate_slope\u001b[0;34m(dates, prices)\u001b[0m\n\u001b[1;32m     13\u001b[0m sum_y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(prices)\n\u001b[1;32m     14\u001b[0m sum_xy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(dates \u001b[38;5;241m*\u001b[39m prices)\n\u001b[0;32m---> 15\u001b[0m sum_x_squared \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(dates \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     17\u001b[0m numerator \u001b[38;5;241m=\u001b[39m N \u001b[38;5;241m*\u001b[39m sum_xy \u001b[38;5;241m-\u001b[39m sum_x \u001b[38;5;241m*\u001b[39m sum_y\n\u001b[1;32m     18\u001b[0m denominator \u001b[38;5;241m=\u001b[39m N \u001b[38;5;241m*\u001b[39m sum_x_squared \u001b[38;5;241m-\u001b[39m sum_x \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/jp_book/lib/python3.11/site-packages/numpy/core/fromnumeric.py:2298\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2295\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   2296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m-> 2298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _wrapreduction(a, np\u001b[38;5;241m.\u001b[39madd, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m, axis, dtype, out, keepdims\u001b[38;5;241m=\u001b[39mkeepdims,\n\u001b[1;32m   2299\u001b[0m                       initial\u001b[38;5;241m=\u001b[39minitial, where\u001b[38;5;241m=\u001b[39mwhere)\n",
      "File \u001b[0;32m~/miniconda3/envs/jp_book/lib/python3.11/site-packages/numpy/core/fromnumeric.py:70\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapreduction\u001b[39m(obj, ufunc, method, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 70\u001b[0m     passkwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     71\u001b[0m                   \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue}\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(obj) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mu\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/jp_book/lib/python3.11/site-packages/numpy/core/fromnumeric.py:70\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapreduction\u001b[39m(obj, ufunc, method, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 70\u001b[0m     passkwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     71\u001b[0m                   \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue}\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(obj) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mu\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import time\n",
    "\n",
    "def calculate_slope(dates, prices):\n",
    "    \"\"\"\n",
    "    计算给定日期和价格的斜率\n",
    "    使用最小二乘法公式计算斜率：slope = (N*Σxy - Σx*Σy) / (N*Σx^2 - (Σx)^2)\n",
    "    \"\"\"\n",
    "    N = len(dates)\n",
    "    sum_x = np.sum(dates)\n",
    "    sum_y = np.sum(prices)\n",
    "    sum_xy = np.sum(dates * prices)\n",
    "    sum_x_squared = np.sum(dates ** 2)\n",
    "\n",
    "    numerator = N * sum_xy - sum_x * sum_y\n",
    "    denominator = N * sum_x_squared - sum_x ** 2\n",
    "\n",
    "    return numerator / denominator if denominator != 0 else 0\n",
    "\n",
    "def detect_flash_crash(stock_data, min_days=5, max_days=10, slope_threshold=2, crash_threshold=-5):\n",
    "    stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n",
    "    stock_data.sort_values(by='Date', inplace=True)\n",
    "    stock_data['Date'] = stock_data['Date'].map(pd.Timestamp.toordinal)  # 将日期转换为整数\n",
    "\n",
    "    results = []\n",
    "    unique_stocks = stock_data['Name'].unique()\n",
    "\n",
    "    for stock in unique_stocks:\n",
    "        current_stock_data = stock_data[stock_data['Name'] == stock]\n",
    "        close_prices = current_stock_data['Close'].values\n",
    "        dates = current_stock_data['Date'].values\n",
    "\n",
    "        # 提取30天内的所有组合\n",
    "        if len(close_prices) >= max_days:\n",
    "            for i in range(len(close_prices) - max_days + 1):\n",
    "                for n in range(min_days, max_days + 1):\n",
    "                    for comb in combinations(range(i, i + max_days), n):\n",
    "                        # 获取组合对应的日期和收盘价\n",
    "                        selected_dates = dates[list(comb)]\n",
    "                        selected_prices = close_prices[list(comb)]\n",
    "\n",
    "                        # 计算斜率，检测上升趋势\n",
    "                        slope = calculate_slope(selected_dates, selected_prices)\n",
    "\n",
    "                        # 如果斜率大于2，认为是显著上升\n",
    "                        if slope > slope_threshold:\n",
    "                            # 检查上升趋势后的10天内是否出现闪崩\n",
    "                            crash_detected = False\n",
    "                            if i + max_days < len(close_prices) - 10:\n",
    "                                future_prices = close_prices[i + max_days:i + max_days + 10]\n",
    "                                # 计算未来10天内的价格变化百分比\n",
    "                                price_drop = (future_prices[0] - future_prices[-1]) / future_prices[0] * 100\n",
    "                                if price_drop < crash_threshold:  # 认为价格下跌超过5%为闪崩\n",
    "                                    crash_detected = True\n",
    "\n",
    "                            if crash_detected:\n",
    "                                results.append((stock, selected_prices, slope, price_drop))\n",
    "\n",
    "    return results\n",
    "\n",
    "# 测试数据（根据你的需求替换为实际的 filtered_data）\n",
    "flash_crash_data = filtered_data[filtered_data['Name'] == \"ALCO\"]\n",
    "\n",
    "# 开始时间\n",
    "start_time = time.time()\n",
    "\n",
    "# 运行函数\n",
    "result = detect_flash_crash(flash_crash_data)\n",
    "\n",
    "# 结束时间\n",
    "end_time = time.time()\n",
    "print(\"代码运行时间：{:.2f}秒\".format(end_time - start_time))\n",
    "\n",
    "# 打印结果\n",
    "for stock, prices, slope, price_drop in result:\n",
    "    print(f'Stock: {stock}, Prices: {prices}, Slope: {slope}, Flash Crash Drop: {price_drop}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d8a1285d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "代码运行时间：1.02秒\n",
      "匹配的结果数量: 1281\n",
      "结果已保存到 result.csv\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def calculate_slope(dates, prices):\n",
    "    \"\"\"\n",
    "    计算给定日期和价格的斜率\n",
    "    使用最小二乘法公式计算斜率：\n",
    "    slope = (N*Σxy - Σx*Σy) / (N*Σx^2 - (Σx)^2)\n",
    "    \"\"\"\n",
    "    dates = np.array(dates)  # 转换为 NumPy 数组\n",
    "    prices = np.array(prices)  # 转换为 NumPy 数组\n",
    "    \n",
    "    N = len(dates)\n",
    "    sum_x = np.sum(dates)\n",
    "    sum_y = np.sum(prices)\n",
    "    sum_xy = np.sum(dates * prices)\n",
    "    sum_x_squared = np.sum(dates ** 2)\n",
    "\n",
    "    numerator = N * sum_xy - sum_x * sum_y\n",
    "    denominator = N * sum_x_squared - sum_x ** 2\n",
    "\n",
    "    return numerator / denominator if denominator != 0 else 0\n",
    "\n",
    "def get_recent_days_data(dates, prices, crash_date, max_days):\n",
    "    \"\"\"\n",
    "    从崩溃日期向前查找不超过 max_days 天的数据\n",
    "    \"\"\"\n",
    "    flash_candidates = []\n",
    "    candidate_prices = []\n",
    "    \n",
    "    # 从崩溃日期往前查找不超过 max_days 的数据\n",
    "    for i in range(len(dates) - 1, -1, -1):\n",
    "        date_diff = (pd.Timestamp(crash_date) - pd.Timestamp(dates[i])).days\n",
    "        if date_diff > 0 and date_diff < max_days:\n",
    "            flash_candidates.append(dates[i])\n",
    "            candidate_prices.append(prices[i])\n",
    "        elif date_diff >= max_days:\n",
    "            break  # 超出 max_days 时停止查找\n",
    "\n",
    "    # 返回按时间顺序排列的数据\n",
    "    return flash_candidates[::-1], candidate_prices[::-1]\n",
    "\n",
    "def detect_flash_crash(stock_data, min_days=5, max_days=10, slope_threshold=0.1, crash_threshold=0.5):\n",
    "    stock_data = stock_data.copy()\n",
    "    stock_data['Date'] = pd.to_datetime(stock_data['Date'], format='%Y-%m-%d')\n",
    "    stock_data.sort_values(by='Date', inplace=True)\n",
    "\n",
    "    result_list = []\n",
    "    unique_stocks = stock_data['Name'].unique()\n",
    "\n",
    "    for stock in unique_stocks:\n",
    "        current_stock_data = stock_data[stock_data['Name'] == stock]\n",
    "        close_prices = current_stock_data['Close'].values\n",
    "        low_prices = current_stock_data['Low'].values\n",
    "        high_prices = current_stock_data['High'].values\n",
    "        dates = current_stock_data['Date'].values\n",
    "\n",
    "        # 遍历每一个可能的崩溃事件\n",
    "        for j in range(len(close_prices)):\n",
    "            price_drop = high_prices[j] - low_prices[j]\n",
    "            if price_drop >= crash_threshold:\n",
    "                crash_date = dates[j]\n",
    "                \n",
    "                # 查找崩溃日期前 max_days 天内的数据\n",
    "                flash_candidates, candidate_prices = get_recent_days_data(dates[:j], close_prices[:j], crash_date, max_days)\n",
    "                \n",
    "                if len(flash_candidates) < min_days:\n",
    "                    continue\n",
    "                \n",
    "                # 衍生约束\n",
    "                if max(candidate_prices) <= low_prices[j] and min(candidate_prices) >= high_prices[j]:\n",
    "                    continue\n",
    "\n",
    "                # 满足 min 和 max 条件后生成唯一组合\n",
    "                for n in range(min_days, min(max_days, len(flash_candidates) + 1)):\n",
    "                    unique_combinations = set(combinations(range(len(flash_candidates)), n))\n",
    "                    for comb in unique_combinations:\n",
    "                        selected_dates = [pd.Timestamp(flash_candidates[i]).toordinal() for i in comb]\n",
    "                        selected_prices = [candidate_prices[i] for i in comb]\n",
    "                        if min(selected_prices) < low_prices[j] or max(selected_prices) > high_prices[j]:\n",
    "                            continue \n",
    "\n",
    "                        # 计算斜率\n",
    "                        slope = calculate_slope(selected_dates, selected_prices)\n",
    "                        if slope >= slope_threshold:\n",
    "                            begin_date = pd.Timestamp(flash_candidates[comb[0]]).strftime('%Y-%m-%d')\n",
    "\n",
    "                            end_date = pd.Timestamp(crash_date).strftime('%Y-%m-%d')\n",
    "\n",
    "                            # flash_stage 仅包括 selected_dates 转换后的日期\n",
    "                            flash_stage = [pd.Timestamp(flash_candidates[i]).strftime('%Y-%m-%d') for i in comb]\n",
    "\n",
    "                            result_dict = {\n",
    "                                \"begin_date\": begin_date,\n",
    "                                \"end_date\": end_date,\n",
    "                                \"flash_stage\": \";\".join(flash_stage),\n",
    "                                \"crash_stage\": pd.Timestamp(crash_date).strftime('%Y-%m-%d')\n",
    "                            }\n",
    "                            if result_dict not in result_list:\n",
    "                                result_list.append(result_dict)\n",
    "                    \n",
    "    return result_list\n",
    "\n",
    "# 假设 filtered_data 是输入数据\n",
    "flash_crash_data = pd.read_csv('ALCO.csv')\n",
    "flash_crash_data.head(5)\n",
    "\n",
    "start_time = time.time()\n",
    "result = detect_flash_crash(flash_crash_data)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"代码运行时间：{end_time - start_time:.2f}秒\")\n",
    "print(f\"匹配的结果数量: {len(result)}\")\n",
    "\n",
    "# 将结果转换为 DataFrame 并保存为 CSV 文件\n",
    "df = pd.DataFrame(result)\n",
    "df.to_csv(\"result.csv\", index=False)\n",
    "\n",
    "print(\"结果已保存到 result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6847632f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始 result_df 行数: 394\n",
      "原始 latency_df 行数: 1282\n",
      "重叠的部分行数（交集）: 394\n",
      "只在 result_df 中的部分行数: 0\n",
      "只在 latency_df 中的部分行数: 888\n",
      "合计行数: 1282\n",
      "预期行数（result_df + latency_df - overlap_df）: 1282\n"
     ]
    }
   ],
   "source": [
    "result_df = pd.DataFrame(result)\n",
    "# result_df = pd.read_csv('lazy.csv')\n",
    "latency_df = pd.read_csv('latency_handle.csv')\n",
    "# 确保数据唯一性\n",
    "result_df = result_df.drop_duplicates().reset_index(drop=True)\n",
    "latency_df = latency_df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# 计算原始行数\n",
    "print(\"原始 result_df 行数:\", len(result_df))\n",
    "print(\"原始 latency_df 行数:\", len(latency_df))\n",
    "\n",
    "# 找到重叠部分（交集）\n",
    "overlap_df = pd.merge(result_df, latency_df, on=['begin_date', 'end_date', 'flash_stage', 'crash_stage'], how='inner')\n",
    "print(\"重叠的部分行数（交集）:\", len(overlap_df))\n",
    "\n",
    "# 找到不重叠部分（差集）\n",
    "unique_to_result_df = pd.concat([result_df, overlap_df]).drop_duplicates(keep=False).reset_index(drop=True)\n",
    "unique_to_latency_df = pd.concat([latency_df, overlap_df]).drop_duplicates(keep=False).reset_index(drop=True)\n",
    "\n",
    "# 输出各部分的行数\n",
    "print(\"只在 result_df 中的部分行数:\", len(unique_to_result_df))\n",
    "print(\"只在 latency_df 中的部分行数:\", len(unique_to_latency_df))\n",
    "\n",
    "# 确认总行数\n",
    "total_rows = len(overlap_df) + len(unique_to_result_df) + len(unique_to_latency_df)\n",
    "print(\"合计行数:\", total_rows)\n",
    "print(\"预期行数（result_df + latency_df - overlap_df）:\", len(result_df) + len(latency_df) - len(overlap_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "a2c986e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1996-03-29;1996-04-01;1996-04-02;1996-04-03;1996-04-04\n"
     ]
    }
   ],
   "source": [
    "# 输出 unique_to_latency_df 的最后一条数据的 'flash_stage' 列\n",
    "print(unique_to_latency_df.tail(1)['flash_stage'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "666985e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>begin_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>flash_stage</th>\n",
       "      <th>crash_stage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1996-03-29</td>\n",
       "      <td>1996-04-08</td>\n",
       "      <td>1996-03-29;1996-04-01;1996-04-02;1996-04-03;19...</td>\n",
       "      <td>1996-04-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   begin_date    end_date                                        flash_stage  \\\n",
       "0  1996-03-29  1996-04-08  1996-03-29;1996-04-01;1996-04-02;1996-04-03;19...   \n",
       "\n",
       "  crash_stage  \n",
       "0  1996-04-08  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_to_latency_df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "05fbed9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "事件不存在于 result_df 中\n",
      "           Date      Low   Open   Volume   High  Close  Adjusted Close  Name\n",
      "4055 1996-03-29  22.5000  23.00    800.0  23.00  22.50       13.490532  ALCO\n",
      "4056 1996-04-01  22.5000  22.50   5000.0  23.25  22.50       13.490532  ALCO\n",
      "4057 1996-04-02  21.9375  22.25  23700.0  23.25  22.75       13.640431  ALCO\n",
      "4058 1996-04-03  23.5000  23.50   1600.0  23.50  23.50       14.090112  ALCO\n",
      "4059 1996-04-04  23.5000  23.50      0.0  23.50  23.50       14.090112  ALCO\n",
      "           Date    Low   Open  Volume  High  Close  Adjusted Close  Name\n",
      "4060 1996-04-08  22.25  22.25  7400.0  23.5   23.5       14.090112  ALCO\n",
      "[22.5  22.5  22.75 23.5  23.5 ]\n",
      "事件检测结果：满足事件条件\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "class FlashCrashTest:\n",
    "    def __init__(self, data, result_df, slope_threshold=0.1):\n",
    "        \"\"\"\n",
    "        初始化测试类\n",
    "        :param data: 股票数据的 DataFrame，需包含 Date, Name, Close 列\n",
    "        :param result_df: 现有事件记录的 DataFrame，用于避免重复检查\n",
    "        :param slope_threshold: 用于检测上升趋势的斜率阈值\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.result_df = result_df\n",
    "        self.data['Date'] = pd.to_datetime(self.data['Date'])\n",
    "        self.result_df['begin_date'] = pd.to_datetime(self.result_df['begin_date'])\n",
    "        self.result_df['end_date'] = pd.to_datetime(self.result_df['end_date'])\n",
    "        self.slope_threshold = slope_threshold\n",
    "\n",
    "    def calculate_slope(self, dates, prices):\n",
    "        \"\"\"\n",
    "        计算斜率\n",
    "        \"\"\"\n",
    "        dates = np.array(dates)\n",
    "        prices = np.array(prices)\n",
    "        \n",
    "        N = len(dates)\n",
    "        sum_x = np.sum(dates)\n",
    "        sum_y = np.sum(prices)\n",
    "        sum_xy = np.sum(dates * prices)\n",
    "        sum_x_squared = np.sum(dates ** 2)\n",
    "\n",
    "        numerator = N * sum_xy - sum_x * sum_y\n",
    "        denominator = N * sum_x_squared - sum_x ** 2\n",
    "\n",
    "        return numerator / denominator if denominator != 0 else 0\n",
    "\n",
    "    def check_event(self, flash_dates_str, crash_date_str):\n",
    "        \"\"\"\n",
    "        检查给定日期中是否发生事件\n",
    "        :param flash_dates_str: Flash阶段的日期字符串，格式为 \"YYYY-MM-DD;YYYY-MM-DD;...\"\n",
    "        :param crash_date_str: Crash阶段的日期字符串，格式为 \"YYYY-MM-DD\"\n",
    "        :return: bool，是否发生事件\n",
    "        \"\"\"\n",
    "        # 解析日期\n",
    "        flash_dates = [datetime.strptime(date.strip(), '%Y-%m-%d') for date in flash_dates_str.split(';')]\n",
    "        crash_date = datetime.strptime(crash_date_str.strip(), '%Y-%m-%d')\n",
    "        begin_date = flash_dates[0]\n",
    "\n",
    "        # 首先检查是否已在 result_df 中\n",
    "        existing_event = self.result_df[\n",
    "            (self.result_df['begin_date'] == begin_date) & (self.result_df['end_date'] == crash_date)&\n",
    "            (self.result_df['flash_stage'] == \";\".join([date.strftime('%Y-%m-%d') for date in flash_dates]))\n",
    "        ]\n",
    "        \n",
    "        if not existing_event.empty:\n",
    "            print(\"事件已存在于 result_df 中\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"事件不存在于 result_df 中\")\n",
    "        # 筛选 ALCO 股票数据\n",
    "        stock_data = self.data[self.data['Name'] == 'ALCO']\n",
    "\n",
    "        # 获取 flash 和 crash 阶段的收盘价数据\n",
    "        flash_data = stock_data[stock_data['Date'].isin(flash_dates)]\n",
    "        crash_data = stock_data[stock_data['Date'] == crash_date]\n",
    "        print(flash_data)\n",
    "        print(crash_data)\n",
    "\n",
    "        if flash_data.empty or crash_data.empty:\n",
    "            print(\"指定的 flash 或 crash 日期数据缺失\")\n",
    "            return False\n",
    "\n",
    "        # 计算 flash 阶段的斜率\n",
    "        flash_ordinal_dates = flash_data['Date'].map(datetime.toordinal).values\n",
    "        flash_prices = flash_data['Close'].values\n",
    "        slope = self.calculate_slope(flash_ordinal_dates, flash_prices)\n",
    "        print(flash_prices)\n",
    "\n",
    "        # 检查是否满足上升斜率和 crash 阶段的价格低于 flash 阶段的最高值\n",
    "        if slope >= self.slope_threshold and crash_data['High'].values[0] >= max(flash_prices) and crash_data['Low'].values[0] <= min(flash_prices):\n",
    "            print(\"事件检测结果：满足事件条件\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"事件检测结果：不满足事件条件\")\n",
    "            return False\n",
    "\n",
    "# 读取数据\n",
    "data = pd.read_csv('ALCO.csv')\n",
    "\n",
    "# 示例输入1987-03-23,1987-04-01,,\n",
    "\n",
    "tester = FlashCrashTest(data, result_df)\n",
    "flash_dates = \"1996-03-29;1996-04-01;1996-04-02;1996-04-03;1996-04-04\"\n",
    "crash_date = \"1996-04-08\"\n",
    "tester.check_event(flash_dates, crash_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d4623f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "      <th>High</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adjusted Close</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1980-03-17</td>\n",
       "      <td>7.625</td>\n",
       "      <td>7.875</td>\n",
       "      <td>8800.0</td>\n",
       "      <td>8.125</td>\n",
       "      <td>7.625</td>\n",
       "      <td>4.285454</td>\n",
       "      <td>ALCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1980-03-18</td>\n",
       "      <td>7.625</td>\n",
       "      <td>7.625</td>\n",
       "      <td>5200.0</td>\n",
       "      <td>8.125</td>\n",
       "      <td>7.625</td>\n",
       "      <td>4.285454</td>\n",
       "      <td>ALCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1980-03-19</td>\n",
       "      <td>7.500</td>\n",
       "      <td>7.625</td>\n",
       "      <td>12400.0</td>\n",
       "      <td>8.000</td>\n",
       "      <td>7.500</td>\n",
       "      <td>4.215203</td>\n",
       "      <td>ALCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1980-03-20</td>\n",
       "      <td>7.375</td>\n",
       "      <td>7.500</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>7.875</td>\n",
       "      <td>7.375</td>\n",
       "      <td>4.144949</td>\n",
       "      <td>ALCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1980-03-21</td>\n",
       "      <td>7.375</td>\n",
       "      <td>7.375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.375</td>\n",
       "      <td>7.375</td>\n",
       "      <td>4.144949</td>\n",
       "      <td>ALCO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date    Low   Open   Volume   High  Close  Adjusted Close  Name\n",
       "0  1980-03-17  7.625  7.875   8800.0  8.125  7.625        4.285454  ALCO\n",
       "1  1980-03-18  7.625  7.625   5200.0  8.125  7.625        4.285454  ALCO\n",
       "2  1980-03-19  7.500  7.625  12400.0  8.000  7.500        4.215203  ALCO\n",
       "3  1980-03-20  7.375  7.500   1600.0  7.875  7.375        4.144949  ALCO\n",
       "4  1980-03-21  7.375  7.375      0.0  7.375  7.375        4.144949  ALCO"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flash_crash_data.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d517ed2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 中没有重复项\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 将字典列表转化为不可变的格式，以便检查重复项\n",
    "result_tuples = [tuple(sorted(item.items())) for item in result]\n",
    "# 使用 Counter 统计出现次数\n",
    "duplicates = [item for item, count in Counter(result_tuples).items() if count > 1]\n",
    "\n",
    "# 输出结果\n",
    "if duplicates:\n",
    "    print(\"Result 中存在重复项：\")\n",
    "    for duplicate in duplicates:\n",
    "        print(dict(duplicate))  # 将元组还原为字典格式输出\n",
    "else:\n",
    "    print(\"Result 中没有重复项\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96c3d401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'begin_date': '1981-06-09',\n",
       " 'end_date': '1981-06-22',\n",
       " 'flash_stage': '1981-06-09;1981-06-10;1981-06-11;1981-06-12;1981-06-15',\n",
       " 'crash_stage': '1981-06-22'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
